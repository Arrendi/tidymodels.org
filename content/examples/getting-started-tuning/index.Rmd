---
title: "Getting Started 4: Tuning model parameters"
tags: [regression models]
---


```{r load, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  digits = 3,
  comment = "#>",
  dev = 'svg', 
  dev.args = list(bg = "transparent")
)
options(width = 100, digits = 3)

library(tidymodels)
library(ranger)
library(modeldata)
library(kableExtra)
theme_set(theme_minimal())
doParallel::registerDoParallel()
```

Some model parameters cannot be learned directly from a data set during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of predictors that are sampled at splits in a tree-based model (we call this `mtry` in tidymodels) or the learning rate in a boosted tree model (we call this `learn_rate`). Instead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on resampled data sets and exploring how well all these models perform. This process is called **tuning**.

## Predicting wine ratings

When you are at the store considering which bottle of wine to buy, often you can notice ratings for the wines, typically displayed with the wines. [*Wine Enthusiast* magazine](https://www.winemag.com/) is a reputable and well-known source for these ratings; they publish ratings on a 100-point scale, although they only post reviews for wines that score equal to or above 80, effectively making the range 80 to 100. This data set was [scraped from the magazine's website in 2017](https://www.kaggle.com/zynicide/wine-reviews), and includes information about each wine such as its country of origin to price to grape varietal and more.

```{r wine-ratings}
library(tidyverse)

wine_ratings <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv")

wine_ratings
```

How are the ratings distributed? What relationship do we see with price?

```{r wine-scatter, dependson="wine-ratings", fig.width=8, fig.height=5}
wine_ratings %>%
  ggplot(aes(price, points)) +
  geom_smooth() +
  geom_jitter(alpha = 0.01) +
  scale_x_log10(labels = scales::dollar_format())
```

We do see evidence for a relationship with price, but it is logarithmic (not linear), there is a large amount of individual variability, and the relationship is less certain above a few hundred dollars. We expect this aligns well with the real-world experience of most individuals when it comes to enjoying wine!

This data set provides us with much more information than only price, and we can build a machine learning model to predict the rating given wine characteristics such as country, winery, and variety. We also know the taster for each review and can account for taster-to-taster differences. Let's build a data set to predict these ratings, and then build training and testing sets.

```{r wine-split, dependson="wine-ratings"}
library(tidymodels)

wine_df <- wine_ratings %>%
  select(points, price, country, province, variety, winery) %>%
  na.omit()

wine_split <- initial_split(wine_df)
wine_train <- training(wine_split)
wine_test  <- testing(wine_split)
```

We don't have that many columns in the `wine_df` dataframe, but many of these features have many, many unique levels. For example, there are `r comma(n_distinct(wine_df$winery))` unique wineries in this data set, a real challenge for many modeling approaches. Also, we expect there will be some [interactions](https://bookdown.org/max/FES/approaches-when-complete-enumeration-is-practically-impossible.html#tree-based-methods) important to wine quality, such as between country/province and variety, but we don't want to code those by hand. Considering these characteristics, let's consider a tree-based model for this problem, such as a random forest model.

## Modeling

To fit a random forest model on the training set, we can use the `parsnip` package together with the `ranger` package. Let's first define our model specification, using mostly the default values for hyperparameters:

```{r rf-spec}
rf_spec <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")
```

From this, we can use the `fit()` function with a straightforward model formula: 

```{r rf-fit, dependson=c("rf-spec", "wine-split")}
set.seed(123)
rf_fit <- rf_spec %>% 
  fit(points ~ ., data = wine_train)

rf_fit
```

We have trained a random forest model!

## Tuning hyperparameters

Random forest models typically perform well with defaults, but there are several hyperparameters that can be tuned for better performance. The two hyperparameters that have the largest impact on predictive accuracy are

- the number of features considered at any given split (which we call `mtry` in tidymodels), and
- the minimum node size for the node to be split further (which we call `min_n`).

To tune these parameters, we can create a new model specification, identifying which hyperparameters we wil `tune()`. Let's also add `importance = "permutation"` to the engine so we can calculate variable importance.

```{r tune-spec}
tune_spec <- 
  rand_forest(
    mtry = tune(),
    trees = 1000,
    min_n = tune()
  ) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

tune_spec
```

We can't train this specification on a single data set and learn what their values should be, but we can train many models using resampled data and see which models turn out best. Let's create cross-validation folds for tuning, and then use `tune_grid()` to fit models at different values for each tuned hyperparameter.

```{r rf-res, dependson=c("tune-spec", "wine-split")}
set.seed(234)
wine_folds <- vfold_cv(wine_train)

rf_res <- tune_spec %>%
  tune_grid(
    points ~ .,
    resamples = wine_folds
  )

rf_res
```

Once we have our tuning results, we can explore them through visualization and we can select the best result.

```{r lowest-rmse, dependson="rf-res", fig.width=8, fig.height=7}
rf_res %>%
  collect_metrics() %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2)
  
lowest_rmse <- rf_res %>%
  select_best("rmse", maximize = FALSE)

lowest_rmse
```

These are the values for `mtry` and `min_n` that minimize RMSE in this data set of wine ratings. We can update (or "finalize") our model with these values.

```{r final-rf, dependson="lowest-rmse"}
final_rf <- finalize_model(
  tune_spec,
  lowest_rmse
)
```

Perhaps we would also like to understand what variables are important in this final model. We can use the [vip](https://koalaverse.github.io/vip/) package to estimate variable importance.

```{r vip, dependson="final-rf"}
library(vip)

final_rf %>%
  set_engine("ranger", importance = "permutation") %>% 
  fit(wine_train) %>%
  vi() %>%
  kable()
```

Finally, let's return to our test data and estimate the model performance we expect to see on new data. We can use the function `last_fit()` with our finalized model; this function _fits_ the finalized model on the training data and _evaluates_ the finalized model on the testing data.

```{r last-fit, dependson=c("final-rf", "wine-split")}
last_fit(final_rf, wine_split) %>%
  collect_metrics
```

The performance metrics from the test set indicate that we did not overfit during our tuning procedure.
