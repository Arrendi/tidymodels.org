---
title: "Subsampling for Class Imbalances"
---

<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<script src="/rmarkdown-libs/elevate-section-attrs/elevate-section-attrs.js"></script>


<p>In addition to the <code>tidymodels</code> package, this example uses the <code>caret</code>, <code>discrim</code>, and <code>mlbench</code> packages.</p>
<p>Subsampling can be a helpful approach to dealing will classification data where one or more classes occur very infrequently. Often, most models will overfit to the majority class and produce very good statistics for the class containing the frequently occurring classes while the minority classes have poor performance.</p>
<p>Consider a two-class problem where the first class has a very low rate of occurrence. The <a href="https://topepo.github.io/caret/"><code>caret</code></a> package has a function that can simulate such data:</p>
<pre class="r"><code>library(caret)

set.seed(244)
imbal_data &lt;- twoClassSim(1000, intercept = 10)
table(imbal_data$Class)
#&gt; 
#&gt; Class1 Class2 
#&gt;     47    953</code></pre>
<p>If “Class1” is the event of interest, it is very likely that a classification model would be able to achieve very good <em>specificity</em> since almost all of the data are the second class. <em>Sensitivity</em> will often be poor since the models will optimize accuracy (or other loss functions) by predicting everything to be the majority class.</p>
<p>When there are two classes, the results is that the default probability cutoff of 50% is inappropriate; a different cutoff that is more extreme might be able to achieve good performance.</p>
<p>One way to alleviate this issue is to <em>subsample</em> the data. There are a number of ways to do this but the most simple one is to <em>sample down</em> the majority class data until it occurs with the same frequency as the minority class. While counterintuitive, throwing out a large percentage of the data can be effective at producing a results. In some cases, this means that the overall performance of the model is better (e.g. improved area under the ROC curve). However, subsampling almost always produces models that are <em>better calibrated</em>, meaning that the distributions of the class probabilities are model well behaved. As a result, the default 50% cutoff is much model likely to produce better sensitivity and specificity values than they would otherwise.</p>
<p>To demonstrate this, <code>step_downsample</code> will be used in a recipe for the simulated data. In terms of workflow:</p>
<ul>
<li>It is extremely important that subsampling occurs <em>inside of resampling</em>. Otherwise, the resampling process can produce <a href="https://topepo.github.io/caret/subsampling-for-class-imbalances.html#resampling">poor estimates of model performance</a>.</li>
<li>The subsampling process should only be applied to the analysis set. The assessment set should reflect the event rates seen “in the wild” and, for this reason, the <code>skip</code> argument to <code>step_downsample</code> is defaulted to <code>TRUE</code>.</li>
</ul>
<p>Here is a simple recipe:</p>
<pre class="r"><code>library(recipes)
imbal_rec &lt;- 
  recipe(Class ~ ., data = imbal_data) %&gt;%
  step_downsample(Class)</code></pre>
<p>Basic cross-validation is used to resample the model:</p>
<pre class="r"><code>library(rsample)
set.seed(5732)
cv_folds &lt;- vfold_cv(imbal_data, strata = &quot;Class&quot;, repeats = 5)</code></pre>
<p>An additional column is added to the data that contains the trained recipes for each resample:</p>
<pre class="r"><code>library(purrr)
cv_folds &lt;- 
  cv_folds %&gt;%
  mutate(recipes = map(splits, prepper, recipe = imbal_rec))
cv_folds$recipes[[1]]
#&gt; Data Recipe
#&gt; 
#&gt; Inputs:
#&gt; 
#&gt;       role #variables
#&gt;    outcome          1
#&gt;  predictor         15
#&gt; 
#&gt; Training data contained 900 data points and no missing data.
#&gt; 
#&gt; Operations:
#&gt; 
#&gt; Down-sampling based on Class [trained]</code></pre>
<p>The model that will be used to demonstrate subsampling is <a href="https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis">quadratic discriminant analysis</a> via the <code>MASS</code> package. A function will be used to train the model and to produce class probabilities as well as hard class predictions using the default 50% cutoff. When a recipe is passed to the function, down-sampling will be applied. If no recipe is given, the data are used to fit the model as-is:</p>
<pre class="r"><code>library(MASS)

assess_res &lt;- function(split, rec = NULL, ...) {
  if (!is.null(rec))
    mod_data &lt;- juice(rec)
  else
    mod_data &lt;- analysis(split)
  
  mod_fit &lt;- qda(Class ~ ., data = mod_data)
  
  if (!is.null(rec))
    eval_data &lt;- bake(rec, assessment(split))
  else
    eval_data &lt;- assessment(split)
  
  eval_data &lt;- eval_data 
  predictions &lt;- predict(mod_fit, eval_data)
  eval_data %&gt;%
    mutate(
      pred = predictions$class,
      prob = predictions$posterior[,1]
    ) %&gt;%
    dplyr::select(Class, pred, prob)
}</code></pre>
<p>For example:</p>
<pre class="r"><code># No subsampling
assess_res(cv_folds$splits[[1]]) %&gt;% head
#&gt;    Class   pred     prob
#&gt; 1 Class2 Class2 7.89e-13
#&gt; 2 Class2 Class2 3.42e-09
#&gt; 3 Class2 Class2 7.55e-27
#&gt; 4 Class2 Class2 1.79e-05
#&gt; 5 Class2 Class2 4.00e-14
#&gt; 6 Class2 Class2 1.63e-09

# With downsampling:
assess_res(cv_folds$splits[[1]], cv_folds$recipes[[1]]) %&gt;% head
#&gt; # A tibble: 6 x 3
#&gt;   Class  pred       prob
#&gt;   &lt;fct&gt;  &lt;fct&gt;     &lt;dbl&gt;
#&gt; 1 Class2 Class2 1.58e-11
#&gt; 2 Class2 Class2 3.31e- 8
#&gt; 3 Class2 Class2 2.59e-24
#&gt; 4 Class2 Class2 5.60e- 4
#&gt; 5 Class2 Class2 1.94e-12
#&gt; 6 Class2 Class2 1.26e- 8</code></pre>
<p>To measure model effectiveness, two metrics are used:</p>
<ul>
<li>The area under the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a> is an overall assessment of performance across <em>all</em> cutoffs. Values near one indicate very good results while values near 0.05 would imply that the model is very poor.</li>
<li>The <em>J</em> index (a.k.a. <a href="https://en.wikipedia.org/wiki/Youden%27s_J_statistic">Youden’s <em>J</em></a> statistic) is <code>sensitivity + specificity - 1</code>. Values near one are once again best.</li>
</ul>
<p>If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the <em>J</em> index would be lower for models with pathological distributions for the class probabilities. The <code>yardstick</code> package will be used to compute these metrics.</p>
<p>Now, we train the models and generate the predictions. These are stored in list columns where each list element is a data frame of the predictions on the assessment data:</p>
<pre class="r"><code>cv_folds &lt;- 
  cv_folds %&gt;%
  mutate(
    sampled_pred = map2(splits, recipes, assess_res),
    normal_pred  =  map(splits, assess_res)
  )
cv_folds
#&gt; #  10-fold cross-validation repeated 5 times using stratification 
#&gt; # A tibble: 50 x 6
#&gt;    splits          id      id2    recipes      sampled_pred     normal_pred     
#&gt;  * &lt;named list&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;named list&gt; &lt;named list&gt;     &lt;named list&gt;    
#&gt;  1 &lt;split [900/10… Repeat1 Fold01 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt;  2 &lt;split [900/10… Repeat1 Fold02 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt;  3 &lt;split [900/10… Repeat1 Fold03 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt;  4 &lt;split [900/10… Repeat1 Fold04 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt;  5 &lt;split [900/10… Repeat1 Fold05 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt;  6 &lt;split [900/10… Repeat1 Fold06 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt;  7 &lt;split [900/10… Repeat1 Fold07 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt;  8 &lt;split [900/10… Repeat1 Fold08 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt;  9 &lt;split [900/10… Repeat1 Fold09 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt; 10 &lt;split [900/10… Repeat1 Fold10 &lt;recipe&gt;     &lt;tibble [100 × … &lt;df[,3] [100 × …
#&gt; # … with 40 more rows</code></pre>
<p>Now, the performance metrics are computed:</p>
<pre class="r"><code>library(yardstick)
cv_folds &lt;- 
  cv_folds %&gt;%
  mutate(
    sampled_roc = 
      map_dfr(sampled_pred, roc_auc, Class, prob) %&gt;% 
      pull(&quot;.estimate&quot;),
    
    normal_roc =  
      map_dfr(normal_pred,  roc_auc, Class, prob) %&gt;% 
      pull(&quot;.estimate&quot;),  
    
    sampled_J =   
      map_dfr(sampled_pred, j_index, Class, pred) %&gt;% 
      pull(&quot;.estimate&quot;),
    
    normal_J =    
      map_dfr(normal_pred,  j_index, Class, pred) %&gt;% 
      pull(&quot;.estimate&quot;)       
  )</code></pre>
<p>What do the ROC values look like? A <a href="https://en.wikipedia.org/wiki/Bland%E2%80%93Altman_plot">Bland-Altman plot</a> can be used to show the differences in the results over the range of results:</p>
<pre class="r"><code>ggplot(cv_folds, 
       aes(x = (sampled_roc + normal_roc)/2, 
           y = sampled_roc - normal_roc)) + 
  geom_point() + 
  geom_hline(yintercept = 0, col = &quot;green&quot;)</code></pre>
<p><img src="/examples/sub-sampling/index_files/figure-html/bland-altman-roc-1.png" width="672" /></p>
<p>There doesn’t appear that subsampling had much of an effect on this metric. The average difference is -0.015, which is fairly small.</p>
<p>For the <em>J</em> statistic, the results show a different story:</p>
<pre class="r"><code>ggplot(cv_folds, 
       aes(x = (sampled_J + normal_J)/2, 
           y =  sampled_J - normal_J)) + 
  geom_point() + 
  geom_hline(yintercept = 0, col = &quot;green&quot;)</code></pre>
<p><img src="/examples/sub-sampling/index_files/figure-html/bland-altman-j-1.png" width="672" /></p>
<p>Almost all of the differences area greater than zero. We can use <code>tidyposterior</code> to do a more formal analysis:</p>
<pre class="r"><code>library(tidyposterior)

# Remove all columns except the resample info and the J indices,
# then fit the Bayesian model
j_mod &lt;- 
  cv_folds %&gt;% 
  dplyr::select(-recipes, -matches(&quot;pred$&quot;), -matches(&quot;roc$&quot;)) %&gt;% 
  perf_mod(seed = 62378, iter = 5000)</code></pre>
<p>A simple plot of the posterior distributions of the <em>J</em> indices for each model shows that there is a real difference; subsampling the data prior to modeling produced better calibrated models:</p>
<pre class="r"><code>j_mod %&gt;%
  tidy(seed = 234) %&gt;%
  ggplot()</code></pre>
<p><img src="Subsampling-tidyposterior.png" /><!-- --></p>
