
---
title: "Comparing Models via Bayesian Analysis and Resampling"
---


```{r load, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  digits = 3,
  collapse = TRUE,
  comment = "#>"
)

library(tidyposterior)
library(tidymodels)
theme_set(theme_bw() + theme(legend.position = "top"))
options(width = 100, digits = 3)
```

In addition to the `tidymodels` package, this example uses the `tidyposterior`, `tune`, `earth`, `rpart`, `kernlab`, and `modeldata` packages. 

## Predicting Compressive Strength of Concrete

These data are used as a case study in Kuhn and Johnson (2013). The outcome is the compressive strength of concrete samples and the predictors are the ingredients used in the mixtures. About the data: 

> Yeh (1998) takes a different approach to modeling concrete mixture experiments. Here, separate experiments from 17 sources with common experimental factors were combined into one "meta-experiment" and the author used neural networks to create predictive models across the whole mixture space. Age was also included in the model. The public version of the data set includes 1030 data points across the different experiments, although Yeh (1998) states that some mixtures were removed from his analysis due to nonstandard conditions. There is no information regarding exactly which mixtures were removed, so the analyses here will use all available data points. 

The data are in the `modeldata` package:

```{r data}
library(tidymodels)
library(modeldata)

data("concrete", package = "modeldata")
names(concrete)
```

To resample the data, two repeats of 10-fold cross-validation are used: 

```{r rs}
set.seed(2708)
folds <- vfold_cv(concrete, repeats = 2)
```

Four models are tuned for these data: random forest, a radial basis function support vector machine, a regression tree, and a multivariate adaptive regression spline. For simplicity, the models are tuned over 20 tuning parameter combinations and the model with the smallest RMSE used used as the final settings. This analysis is cursory; in practice, a more rigorous analysis would be conducted and the models would be more extensively characterized.

The `parsnip` model objects are:

```{r parsnip}
svm_mod <- 
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("kernlab")

rf_mod <- 
  rand_forest(mtry = tune(), trees = 1000, min_n = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")

cart_mod <- 
  decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("rpart")

mars_mod <- 
  mars(num_terms = tune(), prod_degree = tune(), prune_method = "none") %>% 
  set_mode("regression") %>% 
  set_engine("earth")
```

The tuning objects are created using grid search:

```{r tune}
set.seed(7053)
svm_tune <- tune_grid(compressive_strength ~ ., svm_mod, resamples = folds, grid = 20)
show_best(svm_tune, metric = "rmse", maximize = FALSE)

set.seed(5401)
rf_tune <- tune_grid(compressive_strength ~ ., rf_mod, resamples = folds, grid = 20)
show_best(rf_tune, metric = "rmse", maximize = FALSE)

set.seed(3336)
cart_tune <- tune_grid(compressive_strength ~ ., cart_mod, resamples = folds, grid = 20)
show_best(cart_tune, metric = "rmse", maximize = FALSE)

mars_grid <- expand.grid(num_terms = 2 * (1:10), prod_degree = 1:2)

set.seed(9313)
mars_tune <- tune_grid(compressive_strength ~ ., mars_mod, resamples = folds, grid = mars_grid)
show_best(mars_tune, metric = "rmse", maximize = FALSE)
```

At this point, we want to extract the individual resampling estimates of RMSE for the best model and then merge them back into the `rsample` object (`folds`). A function is used to do this for each model. These values are merged and then merged back into `folds`: 

```{r extract-rmse, message = FALSE}
extract_rmse <- function(x) {
  # Pull off the name of the argument to extract the model prefix
  cl <- match.call()
  model_name <- deparse(cl[["x"]])
  model_name <- gsub("_tune", "", model_name)
  
  collect_metrics(x, summarize = FALSE) %>% 
    # Keep the numerically best results
    inner_join(select_best(x, metric = "rmse", maximize = FALSE)) %>% 
    dplyr::filter(.metric == "rmse") %>% 
    dplyr::select(.estimate, id, id2) %>% 
    # Rename the RMSE column with the model prefix
    setNames(c(model_name, "id", "id2"))
}

rmse_values <- 
  extract_rmse(svm_tune) %>% 
  inner_join(extract_rmse(rf_tune),   by = c("id", "id2")) %>% 
  inner_join(extract_rmse(cart_tune), by = c("id", "id2")) %>% 
  inner_join(extract_rmse(mars_tune), by = c("id", "id2")) %>% 
  arrange(id, id2) 

folds <- bind_cols(arrange(folds, id, id2), rmse_values %>% dplyr::select(-id, -id2))
```

A tibble of the stacked resampling estimates is saved to use for plotting and diagnostics: 

```{r stacked}
stacked_rmse <- 
  rmse_values %>%
  pivot_longer(
    cols = c(-starts_with("id")),
    names_to = "model",
    values_to = "rmse"
  )
```

Before proceeding, do the `r nrow(folds)` estimates appear to follow a normal distribution? 

```{r qq}
stacked_rmse %>% 
  ggplot(aes(sample = rmse, col = model)) + 
  stat_qq() + 
  stat_qq_line(alpha = .4) + 
  facet_wrap(~model)
```

Not perfect but pretty indicative of normality for a sample size of `r nrow(folds)`. This basically shows the effect of the central limit theorem. 


## A First Bayesian Model

It might make sense to use a probability model that is consistent with the characteristics of the data (in terms of skewness). Instead of using a symmetric distribution for the summary statistics (such as Gaussian), a potentially right skewed probability model might make more theoretical sense. A Gamma distribution is a reasonable choice and can be fit using the generalized linear model embedded in `perf_mod()`. This also requires a _link_ function to be chosen to model the data. The canonical link for this distribution is the inverse transformation and this will be our choice. 

To fit this model, the `family` argument to `stan_glmer()` can be passed in. The default link is the inverse and no extra transformation will be used. 

```{r gamma-fit, include = FALSE}
library(tidyposterior)

gamma_model <-
  perf_mod(
    folds,
    # Now options to `stan_glmer()`:
    family = Gamma(),
    seed = 74,
    iter = 2000,
    chains = 4,
    cores = 4
  )
```
```{r gamma-stats}
# Get the posterior distributions of the mean parameters:
gamma_post <- tidy(gamma_model, seed = 3750)
gamma_mean <- summary(gamma_post)
gamma_mean
```

Are these values consistent with the data? Let's look at the posterior distribution _for the mean RMSE_ and overlay the individual RMSE resamples: 

```{r gamma}
gamma_post %>% 
  as_tibble() %>% 
  ggplot(aes(x = posterior, fill = model)) + 
  geom_histogram(bins = 30, col = "white") + 
  facet_wrap(~ model) + 
  geom_rug(data = stacked_rmse, aes(x = rmse)) + 
  ggtitle("RMSE values assumed Gamma") + 
  theme(legend.position = "none")
```

The spread of the posterior is not much smaller than the range of the original values (which seems excessive). 

## Transforming the Data

Another approach is to transform the RMSE values to something model symmetric and model the data on a different scale. A log transform will be used here using the built-in object `ln_trans`. In using this option, the posterior distributions are computed on the log scale and is automatically back-transformed into the original units. By not passing `family` to the function, we are using a Gaussian model.


```{r log-linear-fit, include = FALSE}
log_linear_model <-
  perf_mod(
    folds,
    transform = ln_trans,
    seed = 74,
    iter = 2000,
    chains = 4,
    cores = 4
  )
```

These results were: 

```{r log-linear}
log_linear_post <- tidy(log_linear_model, seed = 3750)

log_linear_post %>% 
  as_tibble() %>% 
  ggplot(aes(x = posterior, fill = model)) + 
  geom_histogram(bins = 30, col = "white") + 
  facet_wrap(~ model) + 
  geom_rug(data = stacked_rmse, aes(x = rmse)) + 
  ggtitle("RMSE values assumed Log-Normal") + 
  theme(legend.position = "none")
```

The posteriors are a somewhat tighter than the Gamma results. 

One note: using the transformed values for the Bayesian analysis means that, when the posterior values are back-transformed, the posterior distributions can have different variances (despite being fit with a common variance model). The differences are subtle here but note the change in the spread of the posteriors as the change with the mean: 

```{r log-lin-spread}
log_linear_post %>% 
  group_by(model) %>% 
  summarize(std_dev = sd(posterior), mean = mean(posterior)) %>% 
  arrange(mean)
```

## A Simple Gaussian Model

Let's try the easiest model that used a linear function and assumes a Gaussian distribution for the RMSE estimates. 

```{r linear-linear-fit, include = FALSE}
linear_model <- perf_mod(folds, seed = 74, iter = 2000, chains = 4, cores = 4)
```
```{r linear-linear}
linear_post <- tidy(linear_model, seed = 3750)

linear_post %>% 
  as_tibble() %>% 
  ggplot(aes(x = posterior, fill = model)) + 
  geom_histogram(bins = 30, col = "white") + 
  facet_wrap(~ model) + 
  geom_rug(data = stacked_rmse, aes(x = rmse)) + 
  ggtitle("RMSE values assumed Normal") + 
  theme(legend.position = "none")
```

These don't look too different from the log-linear model. One thing is different though: these distributions have about the same spread:

```{r lin-spread}
linear_post %>% 
  group_by(model) %>% 
  summarize(std_dev = sd(posterior), mean = mean(posterior)) %>% 
  arrange(mean)
```

If we want to fit a model where each of the posterior distributions are allowed to have differences variances, the option `hetero_var = TRUE` can be used. 

## Comparing Models

We can compare models using the `contrast_models` function. The function has arguments for two sets of models to compare but if these are left to their default (`NULL`), all pair-wise combinations are used. Let's say that an RMSE difference of 1 unit is important. 

```{r contrast}
one_contrast <- contrast_models(linear_model, list_1 = "rf", list_2 = "svm", seed = 8967)
ggplot(one_contrast, size = 1) + 
  xlab("Posterior for Random Forest - SVM")
summary(one_contrast, size = 1)
````

The negative values indicate that the random forest model tended to have smaller RMSE values than the SVM model. While the probability that the SVM  model is better is effectively zero (in the `probability` column), the region of practical equivalence captures most of the posterior distribution. The probability that the two models are _practically equivalent_ for an effect size of one unit is `r round(summary(one_contrast, size = 1)$pract_equiv* 100, 2)`%.

## One Final Note

The Bayesian models have population parameters for the model effects (akin to "fixed" effects in mixed models) as well as variance parameter(s) related to the resamples. The posteriors computed by this package only reflect the mean parameters and should only be used to make inferences about this data set generally. This posterior calculation could not be used to predict the level of performance for a model on a new _resample_ of the data. In this case, the variance parameters come into play and the posterior would be much wider. 

In essence, the posteriors shown here are measuring the average performance value instead of a resample-specific value.  

 
