---
title: "Time Series Analysis Example"
---

<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<script src="/rmarkdown-libs/elevate-section-attrs/elevate-section-attrs.js"></script>


<p>In addition to the <code>tidymodels</code> package, this example uses the <code>timetk</code>, <code>forecast</code>, <code>sweep</code>, <code>modeldata</code>, and <code>zoo</code> packages.</p>
<p>“<a href="http://www.business-science.io/code-tools/2017/10/25/demo_week_sweep.html">Demo Week: Tidy Forecasting with <code>sweep</code></a>” is an excellent article that uses tidy methods with time series. This article uses their analysis with <code>rsample</code> to get performance estimates for future observations using <a href="https://robjhyndman.com/hyndsight/crossvalidation/">rolling forecast origin resampling</a>.</p>
<p>The data are sales of alcoholic beverages originally from <a href="https://fred.stlouisfed.org/series/S4248SM144NCEN">the Federal Reserve Bank of St. Louis website</a>.</p>
<pre class="r"><code>library(tidymodels)
library(modeldata)
data(&quot;drinks&quot;)
str(drinks, give.att = FALSE)</code></pre>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    309 obs. of  2 variables:
##  $ date          : Date, format: &quot;1992-01-01&quot; &quot;1992-02-01&quot; ...
##  $ S4248SM144NCEN: num  3459 3458 4002 4564 4221 ...</code></pre>
<p>Each row is a month of sales (in millions of US dollars).</p>
<p>Suppose that predictions for one year ahead were needed and the model should use the most recent data from the last 20 years. To setup this resampling scheme:</p>
<pre class="r"><code>roll_rs &lt;- rolling_origin(
  drinks, 
  initial = 12 * 20, 
  assess = 12,
  cumulative = FALSE
  )
nrow(roll_rs)</code></pre>
<pre><code>## [1] 58</code></pre>
<pre class="r"><code>roll_rs</code></pre>
<pre><code>## # Rolling origin forecast resampling 
## # A tibble: 58 x 2
##    splits           id     
##    &lt;list&gt;           &lt;chr&gt;  
##  1 &lt;split [240/12]&gt; Slice01
##  2 &lt;split [240/12]&gt; Slice02
##  3 &lt;split [240/12]&gt; Slice03
##  4 &lt;split [240/12]&gt; Slice04
##  5 &lt;split [240/12]&gt; Slice05
##  6 &lt;split [240/12]&gt; Slice06
##  7 &lt;split [240/12]&gt; Slice07
##  8 &lt;split [240/12]&gt; Slice08
##  9 &lt;split [240/12]&gt; Slice09
## 10 &lt;split [240/12]&gt; Slice10
## # … with 48 more rows</code></pre>
<p>Each <code>split</code> element contains the information about that resample:</p>
<pre class="r"><code>roll_rs$splits[[1]]</code></pre>
<pre><code>## &lt;240/12/309&gt;</code></pre>
<p>For plotting, let’s index each split by the first day of the assessment set:</p>
<pre class="r"><code>get_date &lt;- function(x) {
  min(assessment(x)$date)
}

start_date &lt;- map(roll_rs$splits, get_date)
roll_rs$start_date &lt;- do.call(&quot;c&quot;, start_date)
head(roll_rs$start_date)</code></pre>
<pre><code>## [1] &quot;2012-01-01&quot; &quot;2012-02-01&quot; &quot;2012-03-01&quot; &quot;2012-04-01&quot; &quot;2012-05-01&quot;
## [6] &quot;2012-06-01&quot;</code></pre>
<p>This resampling scheme has 58 splits of the data so that there will be 58 ARIMA models that are fit. To create the models, the <code>auto.arima()</code> function from the <code>forecast</code> package is used. The functions <code>analysis()</code> and <code>assessment()</code> return the data frame, so another step converts the data in to a <code>ts</code> object called <code>mod_dat</code> using a function in the <code>timetk</code> package.</p>
<pre class="r"><code>library(forecast)  # for `auto.arima`
library(timetk)    # for `tk_ts`
library(zoo)       # for `as.yearmon`

fit_model &lt;- function(x, ...) {
  # suggested by Matt Dancho:
  x %&gt;%
    analysis() %&gt;%
    # Since the first day changes over resamples, adjust it
    # based on the first date value in the data frame 
    tk_ts(start = .$date[[1]] %&gt;% as.yearmon(), 
          frequency = 12, 
          silent = TRUE) %&gt;%
    auto.arima(...)
}</code></pre>
<p>Each model is saved in a new column:</p>
<pre class="r"><code>roll_rs$arima &lt;- map(roll_rs$splits, fit_model)

# For example:
roll_rs$arima[[1]]</code></pre>
<pre><code>## Series: . 
## ARIMA(4,1,1)(0,1,2)[12] 
## 
## Coefficients:
##          ar1     ar2    ar3     ar4     ma1    sma1    sma2
##       -0.185  -0.024  0.358  -0.152  -0.831  -0.193  -0.324
## s.e.   0.147   0.166  0.144   0.081   0.138   0.067   0.064
## 
## sigma^2 estimated as 72198:  log likelihood=-1591
## AIC=3198   AICc=3199   BIC=3226</code></pre>
<p>(There are some warnings produced by these first regarding extra columns in the data that can be ignored)</p>
<p>Using the model fits, performance will be measured in two ways:</p>
<ul>
<li><em>interpolation</em> error will measure how well the model fits to the data that were used to create the model. This is most likely optimistic since no holdout method is used.</li>
<li><em>extrapolation</em> or <em>forecast</em> error evaluates the efficacy of the model on the data from the following year (that were not used in the model fit).</li>
</ul>
<p>In each case, the mean absolute percent error (MAPE) is the statistic used to characterize the model fits. The interpolation error can be computed from the <code>Arima</code> object. to make things easy, the <code>sweep</code> package’s <code>sw_glance</code> function is used:</p>
<pre class="r"><code>library(sweep)

roll_rs$interpolation &lt;- map_dbl(
  roll_rs$arima,
  function(x) 
    sw_glance(x)[[&quot;MAPE&quot;]]
  )
summary(roll_rs$interpolation)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    2.84    2.92    2.95    2.95    2.97    3.13</code></pre>
<p>For the extrapolation error, the model and split objects are required. Using these:</p>
<pre class="r"><code>get_extrap &lt;- function(split, mod) {
  n &lt;- nrow(assessment(split))
  # Get assessment data
  pred_dat &lt;- assessment(split) %&gt;%
    mutate(
      pred = as.vector(forecast(mod, h = n)$mean),
      pct_error = ( S4248SM144NCEN - pred ) / S4248SM144NCEN * 100
    )
  mean(abs(pred_dat$pct_error))
}

roll_rs$extrapolation &lt;- 
  map2_dbl(roll_rs$splits, roll_rs$arima, get_extrap)

summary(roll_rs$extrapolation)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    2.37    3.23    3.63    3.65    4.11    5.45</code></pre>
<p>What do these error estimates look like over time?</p>
<pre class="r"><code>roll_rs %&gt;%
  select(interpolation, extrapolation, start_date) %&gt;%
  as.data.frame %&gt;%
  gather(error, MAPE, -start_date) %&gt;%
  ggplot(aes(x = start_date, y = MAPE, col = error)) + 
  geom_point() + 
  geom_line()</code></pre>
<p><img src="/examples/time-series/index_files/figure-html/plot-1.png" width="672" /></p>
<p>It is likely that the interpolation error is an underestimate to some degree.</p>
<p>It is also worth noting that <code>rolling_origin()</code> can be used over calendar periods, rather than just over a fixed window size. This is especially useful for irregular series where a fixed window size might not make sense because of missing data points, or because of calendar features like different months having a different number of days.</p>
<p>The example below demonstrates this idea by splitting <code>drinks</code> into a nested set
of 26 years, and rolling over years rather than months. Note that the end result
accomplishes a different task than the original example, in this case, each slice
moves forward an entire year, rather than just one month.</p>
<pre class="r"><code># The idea is to nest by the period to roll over,
# which in this case is the year.
roll_rs_annual &lt;- drinks %&gt;%
  mutate(year = as.POSIXlt(date)$year + 1900) %&gt;%
  nest(data = c(date, S4248SM144NCEN)) %&gt;%
  rolling_origin(
    initial = 20, 
    assess = 1, 
    cumulative = FALSE
  )

analysis(roll_rs_annual$splits[[1]])</code></pre>
<pre><code>## # A tibble: 20 x 2
##     year           data
##    &lt;dbl&gt; &lt;list&lt;df[,2]&gt;&gt;
##  1  1992       [12 × 2]
##  2  1993       [12 × 2]
##  3  1994       [12 × 2]
##  4  1995       [12 × 2]
##  5  1996       [12 × 2]
##  6  1997       [12 × 2]
##  7  1998       [12 × 2]
##  8  1999       [12 × 2]
##  9  2000       [12 × 2]
## 10  2001       [12 × 2]
## 11  2002       [12 × 2]
## 12  2003       [12 × 2]
## 13  2004       [12 × 2]
## 14  2005       [12 × 2]
## 15  2006       [12 × 2]
## 16  2007       [12 × 2]
## 17  2008       [12 × 2]
## 18  2009       [12 × 2]
## 19  2010       [12 × 2]
## 20  2011       [12 × 2]</code></pre>
<p>The workflow to access these calendar slices is to use <code>bind_rows()</code> to join
each analysis set together.</p>
<pre class="r"><code>mutate(
  roll_rs_annual,
  extracted_slice = map(splits, ~ bind_rows(analysis(.x)$data))
)</code></pre>
<pre><code>## # Rolling origin forecast resampling 
## # A tibble: 6 x 3
##   splits         id     extracted_slice   
## * &lt;list&gt;         &lt;chr&gt;  &lt;list&gt;            
## 1 &lt;split [20/1]&gt; Slice1 &lt;tibble [240 × 2]&gt;
## 2 &lt;split [20/1]&gt; Slice2 &lt;tibble [240 × 2]&gt;
## 3 &lt;split [20/1]&gt; Slice3 &lt;tibble [240 × 2]&gt;
## 4 &lt;split [20/1]&gt; Slice4 &lt;tibble [240 × 2]&gt;
## 5 &lt;split [20/1]&gt; Slice5 &lt;tibble [240 × 2]&gt;
## 6 &lt;split [20/1]&gt; Slice6 &lt;tibble [240 × 2]&gt;</code></pre>
