---
title: "Tidy bootstrapping"
---

<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<script src="/rmarkdown-libs/elevate-section-attrs/elevate-section-attrs.js"></script>


<p>Another place where combining model fits in a tidy way becomes useful is when performing bootstrapping or permutation tests. These approaches have been explored before, for instance by <a href="http://rstudio-pubs-static.s3.amazonaws.com/19698_a4c472606e3c43e4b94720506e49bb7b.html">Andrew MacDonald here</a>, and <a href="https://github.com/hadley/dplyr/issues/269">Hadley has explored efficient support for bootstrapping</a> as a potential enhancement to dplyr. broom fits naturally with dplyr in performing these analyses.</p>
<p>Bootstrapping consists of randomly sampling a dataset with replacement, then performing the analysis individually on each bootstrapped replicate. The variation in the resulting estimate is then a reasonable approximation of the variance in our estimate.</p>
<p>Let’s say we want to fit a nonlinear model to the weight/mileage relationship in the <code>mtcars</code> dataset.</p>
<pre class="r"><code>library(tidymodels)

ggplot(mtcars, aes(mpg, wt)) + 
    geom_point()</code></pre>
<p><img src="/examples/tidy-bootstrapping/index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We might use the method of nonlinear least squares (via the <code>nls()</code> function) to fit a model.</p>
<pre class="r"><code>nlsfit &lt;- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))
summary(nlsfit)
#&gt; 
#&gt; Formula: mpg ~ k/wt + b
#&gt; 
#&gt; Parameters:
#&gt;   Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; k   45.829      4.249  10.786 7.64e-12 ***
#&gt; b    4.386      1.536   2.855  0.00774 ** 
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 2.774 on 30 degrees of freedom
#&gt; 
#&gt; Number of iterations to convergence: 1 
#&gt; Achieved convergence tolerance: 2.877e-08

ggplot(mtcars, aes(wt, mpg)) +
    geom_point() +
    geom_line(aes(y = predict(nlsfit)))</code></pre>
<p><img src="/examples/tidy-bootstrapping/index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>While this does provide a p-value and confidence intervals for the parameters, these are based on model assumptions that may not hold in real data. Bootstrapping is a popular method for providing confidence intervals and predictions that are more robust to the nature of the data.</p>
<p>We can use the <code>bootstraps()</code> function in the <code>rsample</code> package to sample bootstrap replications. First, we construct 100 bootstrap replications of the data, each of which has been randomly sampled with replacement. The resulting object is an <code>rset</code>, which is a dataframe with a column of <code>rsplit</code> objects.</p>
<p>An <code>rsplit</code> object has two main components: an analysis dataset and an assessment dataset, accessible via <code>analysis(rsplit)</code> and <code>assessment(rsplit)</code> respectively. For bootstrap samples, the analysis dataset is the bootstrap sample itself, and the assessment dataset consists of all the out of bag samples.</p>
<pre class="r"><code>set.seed(27)

boots &lt;- bootstraps(mtcars, times = 100)
boots
#&gt; # Bootstrap sampling 
#&gt; # A tibble: 100 x 2
#&gt;    splits          id          
#&gt;    &lt;list&gt;          &lt;chr&gt;       
#&gt;  1 &lt;split [32/13]&gt; Bootstrap001
#&gt;  2 &lt;split [32/10]&gt; Bootstrap002
#&gt;  3 &lt;split [32/13]&gt; Bootstrap003
#&gt;  4 &lt;split [32/11]&gt; Bootstrap004
#&gt;  5 &lt;split [32/9]&gt;  Bootstrap005
#&gt;  6 &lt;split [32/10]&gt; Bootstrap006
#&gt;  7 &lt;split [32/11]&gt; Bootstrap007
#&gt;  8 &lt;split [32/13]&gt; Bootstrap008
#&gt;  9 &lt;split [32/11]&gt; Bootstrap009
#&gt; 10 &lt;split [32/11]&gt; Bootstrap010
#&gt; # … with 90 more rows</code></pre>
<p>We create a helper function to fit an <code>nls</code> model on each bootstrap sample, and then use <code>purrr::map()</code> to apply this function to all the bootstrap samples at once. Similarly, we create a column of tidy coefficient information by unnesting.</p>
<pre class="r"><code>fit_nls_on_bootstrap &lt;- function(split) {
    nls(mpg ~ k / wt + b, analysis(split), start = list(k = 1, b = 0))
}

boot_models &lt;-
  boots %&gt;% 
  mutate(model = map(splits, fit_nls_on_bootstrap),
         coef_info = map(model, tidy))

boot_coefs &lt;- 
  boot_models %&gt;% 
  unnest(coef_info)</code></pre>
<p>The unnested coefficient information contains a summary of each replication combined in a single data frame:</p>
<pre class="r"><code>boot_coefs
#&gt; # A tibble: 200 x 8
#&gt;    splits         id           model term  estimate std.error statistic  p.value
#&gt;    &lt;list&gt;         &lt;chr&gt;        &lt;lis&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
#&gt;  1 &lt;split [32/13… Bootstrap001 &lt;nls&gt; k        42.1       4.05     10.4  1.91e-11
#&gt;  2 &lt;split [32/13… Bootstrap001 &lt;nls&gt; b         5.39      1.43      3.78 6.93e- 4
#&gt;  3 &lt;split [32/10… Bootstrap002 &lt;nls&gt; k        49.9       5.66      8.82 7.82e-10
#&gt;  4 &lt;split [32/10… Bootstrap002 &lt;nls&gt; b         3.73      1.92      1.94 6.13e- 2
#&gt;  5 &lt;split [32/13… Bootstrap003 &lt;nls&gt; k        37.8       2.68     14.1  9.01e-15
#&gt;  6 &lt;split [32/13… Bootstrap003 &lt;nls&gt; b         6.73      1.17      5.75 2.78e- 6
#&gt;  7 &lt;split [32/11… Bootstrap004 &lt;nls&gt; k        45.6       4.45     10.2  2.70e-11
#&gt;  8 &lt;split [32/11… Bootstrap004 &lt;nls&gt; b         4.75      1.62      2.93 6.38e- 3
#&gt;  9 &lt;split [32/9]&gt; Bootstrap005 &lt;nls&gt; k        43.6       4.63      9.41 1.85e-10
#&gt; 10 &lt;split [32/9]&gt; Bootstrap005 &lt;nls&gt; b         5.89      1.68      3.51 1.44e- 3
#&gt; # … with 190 more rows</code></pre>
<p>We can then calculate confidence intervals (using what is called the <a href="https://www.uvm.edu/~dhowell/StatPages/Randomization%20Tests/ResamplingWithR/BootstMeans/bootstrapping_means.html">percentile method</a>):</p>
<pre class="r"><code>alpha &lt;- .05
boot_coefs %&gt;% 
    group_by(term) %&gt;%
    summarize(low = quantile(estimate, alpha / 2),
              high = quantile(estimate, 1 - alpha / 2))
#&gt; # A tibble: 2 x 3
#&gt;   term     low  high
#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;
#&gt; 1 b      0.283  6.74
#&gt; 2 k     38.5   57.6</code></pre>
<p>Or we can use histograms to get a more detailed idea of the uncertainty in each estimate:</p>
<pre class="r"><code>ggplot(boot_coefs, aes(estimate)) + 
    geom_histogram(binwidth = 2) + 
    facet_wrap(~ term, scales = &quot;free&quot;)</code></pre>
<p><img src="/examples/tidy-bootstrapping/index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Or we can use <code>augment</code> to visualize the uncertainty in the curve:</p>
<pre class="r"><code>boot_aug &lt;- 
  boot_models %&gt;% 
  mutate(augmented = map(model, augment)) %&gt;% 
  unnest(augmented)

boot_aug
#&gt; # A tibble: 3,200 x 8
#&gt;    splits          id           model coef_info         mpg    wt .fitted .resid
#&gt;    &lt;list&gt;          &lt;chr&gt;        &lt;lis&gt; &lt;list&gt;          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
#&gt;  1 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  18.7  3.44    17.6  1.08 
#&gt;  2 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  32.4  2.2     24.5  7.89 
#&gt;  3 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  15.5  3.52    17.3 -1.84 
#&gt;  4 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  22.8  3.15    18.7  4.05 
#&gt;  5 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  24.4  3.19    18.6  5.82 
#&gt;  6 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  30.4  1.62    31.4 -1.04 
#&gt;  7 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  10.4  5.42    13.1 -2.75 
#&gt;  8 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  21    2.62    21.4 -0.448
#&gt;  9 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  19.2  3.84    16.3  2.87 
#&gt; 10 &lt;split [32/13]&gt; Bootstrap001 &lt;nls&gt; &lt;tibble [2 × 5…  21    2.62    21.4 -0.448
#&gt; # … with 3,190 more rows</code></pre>
<pre class="r"><code>ggplot(boot_aug, aes(wt, mpg)) +
    geom_point() +
    geom_line(aes(y = .fitted, group = id), alpha=.2)</code></pre>
<p><img src="/examples/tidy-bootstrapping/index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>With only a few small changes, we could easily perform bootstrapping with other kinds of predictive or hypothesis testing models, since the <code>tidy</code> and <code>augment</code> functions works for many statistical outputs. As another example, we could use <code>smooth.spline</code>, which fits a cubic smoothing spline to data:</p>
<pre class="r"><code>fit_spline_on_bootstrap &lt;- function(split) {
    data &lt;- analysis(split)
    smooth.spline(data$wt, data$mpg, df = 4)
}

boot_splines &lt;- 
  boots %&gt;% 
  mutate(spline = map(splits, fit_spline_on_bootstrap),
         aug_train = map(spline, augment))

splines_aug &lt;- 
  boot_splines %&gt;% 
  unnest(aug_train)

ggplot(splines_aug, aes(x, y)) +
  geom_point() +
  geom_line(aes(y = .fitted, group = id), alpha = 0.2)</code></pre>
<p><img src="/examples/tidy-bootstrapping/index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
