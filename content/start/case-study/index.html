<script src="index_files/header-attrs-2.1/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#data-spending">Data Spending</a></li>
<li><a href="#a-first-model-logistic-regression">A first model: logistic regression</a></li>
<li><a href="#tree-based-ensembles">Tree-based ensembles</a></li>
<li><a href="#test-set-results">Test set results</a></li>
<li><a href="#session-information">Session information</a></li>
</ul>
</div>

<p>This article requires that you have the following packages installed: glmnet, ranger, readr, tidymodels, and vip.</p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The previous <em>Getting Started</em> articles have been focused on single tasks related to modeling. This example is a broader case study of building a predictive model. It uses all of the previous topics.</p>
</div>
<div id="data-spending" class="section level1">
<h1>Data Spending</h1>
<p>The data can be found on the <code>tidymodels.org</code> site:</p>
<pre class="r"><code>library(tidymodels)
library(readr)

hotels &lt;- 
  readr::read_csv(&#39;https://bit.ly/hotel_booking_data&#39;) %&gt;%
  mutate_if(is.character, as.factor) 

dim(hotels)
#&gt; [1] 50000    23

table(hotels$children)/nrow(hotels)
#&gt; 
#&gt; children     none 
#&gt;  0.08076  0.91924</code></pre>
<pre class="r"><code>set.seed(35225)
splits &lt;- initial_split(hotels, strata = children)

hotel_train &lt;- training(splits)
hotel_test  &lt;- testing(splits)</code></pre>
<pre class="r"><code>set.seed(948)
val_set &lt;- validation_split(hotel_train, strata = children, prop = 0.80)
val_set
#&gt; # Validation Set Split (0.8/0.2)  using stratification 
#&gt; # A tibble: 1 x 2
#&gt;   splits             id        
#&gt;   &lt;named list&gt;       &lt;chr&gt;     
#&gt; 1 &lt;split [30K/7.5K]&gt; validation</code></pre>
</div>
<div id="a-first-model-logistic-regression" class="section level1">
<h1>A first model: logistic regression</h1>
<p>It makes sense to start with a simple model. Since the outcome is categorical, a logistic regression model would be a good first step. Specifically, a glmnet model is used to potentially add some amount of feature selection during model training. This method of estimating the logistic regression slope parameters uses a <em>penalty</em> on the process so that less relavant predictors are driven towards a value of zero. One of the glmnet penalization methods, called the lasso method, can set the predictor slopes to absolute zero if a large enough penalty is used.</p>
<p>To specify a penalized logistic regression model that uses a feature selection penalty:</p>
<pre class="r"><code>lr_mod &lt;- 
  logistic_reg(penalty = tune(), mixture = 1) %&gt;% 
  set_engine(&quot;glmnet&quot;)</code></pre>
<p>Setting <code>mixture</code> to a value of one means that the glmnet model will focus on potentially removing irrelevant predictors.</p>
<p>To prepare the data for the model, the categorical predictors (e.g., ) should be converted to dummy variables. Additionally, it might make sense fo create a set of date-based predictors that reflect important components related to the arrival date. First, <code>step_date()</code> creates predictors for the year, month, and day of the week. Secondly, <code>step_holiday()</code> generates a set of indicator variables for specific holidays. The addition of <code>step_zv()</code> means that no indicators variables that only contains a single unique value (e.g. all zeros) will be added to the model. This is important because, for penalized models, the the predictors should be centered and scaled.</p>
<p>The recipe for these steps is:</p>
<pre class="r"><code>holidays &lt;- c(&quot;AllSouls&quot;, &quot;AshWednesday&quot;, &quot;ChristmasEve&quot;, &quot;Easter&quot;, 
              &quot;ChristmasDay&quot;, &quot;GoodFriday&quot;, &quot;NewYearsDay&quot;, &quot;PalmSunday&quot;)

lr_recipe &lt;- 
  recipe(children ~ ., data = hotel_train) %&gt;% 
  step_date(arrival_date) %&gt;% 
  step_holiday(arrival_date, holidays = holidays) %&gt;% 
  step_rm(arrival_date) %&gt;% 
  step_dummy(all_nominal(), -all_outcomes()) %&gt;% 
  step_zv(all_predictors()) %&gt;% 
  step_normalize(all_predictors())</code></pre>
<p>The model and recipe can be bundled into a single workflow object to make management of the R objects easy:</p>
<pre class="r"><code>lr_workflow &lt;- 
  workflow() %&gt;% 
  add_model(lr_mod) %&gt;% 
  add_recipe(lr_recipe)</code></pre>
<p>Finally a grid of penalty values are created and the model is tuned. The validation set predictions are saved (via the call to <code>control_grid()</code> below) so that diagnostic information can be available after the model fit. Also, the area under the ROC curve is once again used to quantify how well the model performs across a continuum of event thresholds (recall that the event rate is very low for these data).</p>
<pre class="r"><code>lr_reg_grid &lt;- expand.grid(penalty = 10^seq(-4, -1, length.out = 30))

tune_ctrl &lt;- control_grid(save_pred = TRUE)
roc_only &lt;- metric_set(roc_auc)

lr_res &lt;- 
  lr_workflow %&gt;% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = tune_ctrl,
            metrics = roc_only)
#&gt; ! validation: recipe: The `x` argument of `as_tibble.matrix()` must have column names ...</code></pre>
<p>The resulting validation set metrics are computed and plotted against the amount of penalization:</p>
<pre class="r"><code>lr_res %&gt;% 
  collect_metrics() %&gt;% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab(&quot;Area under the ROC Curve&quot;) +
  scale_x_log10()</code></pre>
<p><img src="figs/logistic-results-1.svg" width="672" /></p>
<p>Performance is generally better when very little penalization is used; this suggests that the majority of the predictors are important to the model. Note the steep drop in the area under the ROC curve that occurs when the amount of penalization is high; this happens because a large enough penalty will remove <em>all</em> predictors from the model.</p>
<p>Since there is a plateau of performance for small penalties, a value closer to the decline in performance is chosen as being best for this model:</p>
<pre class="r"><code>lr_best &lt;- 
  lr_res %&gt;% 
  collect_metrics() %&gt;% 
  arrange(desc(mean)) %&gt;% 
  slice(8)
lr_best
#&gt; # A tibble: 1 x 6
#&gt;    penalty .metric .estimator  mean     n std_err
#&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
#&gt; 1 0.000530 roc_auc binary     0.871     1      NA</code></pre>
<p>This may result in some predictors still being removed form the model despite the small values of the penalty being best.</p>
<p>For this specific penalty value, the validation set ROC curve is:</p>
<pre class="r"><code>lr_auc &lt;- 
  lr_res %&gt;% 
  collect_predictions(parameters = lr_best) %&gt;% 
  roc_curve(children, .pred_children) %&gt;% 
  mutate(model = &quot;Logistic Regression&quot;)

autoplot(lr_auc)</code></pre>
<p><img src="figs/logistic-roc-curve-1.svg" width="672" /></p>
<p>The level of performance generated by this logistic regression model is good but not groundbreaking. Perhaps the linear nature of the prediction equation is too limiting for this data set.</p>
<p>As a next step, we might consider a highly non-linear model generated using tree-based methods.</p>
</div>
<div id="tree-based-ensembles" class="section level1">
<h1>Tree-based ensembles</h1>
<p>Once effective and low-maintenance modeling technique is <em>random forest</em> (used in a previous vignette). This model will be used with less pre-processing than the logistic regression. The conversion to dummy variables and variable normalization are not required. As before, the date predictor is engineered so that the random forest model does not need to work hard to tease these potential patterns from the data.</p>
<pre class="r"><code>rf_recipe &lt;- 
  recipe(children ~ ., data = hotel_train) %&gt;% 
  step_date(arrival_date) %&gt;% 
  step_holiday(arrival_date) %&gt;% 
  step_rm(arrival_date) </code></pre>
<p>The computations required for model tuning can usually be easily parallelized. However, when the models are resampled, the most efficient approach is to parallelize the resampling process. In this case study, a single validation set is being used so parallelization isn’t an option using the tune package. Despite this, the ranger package can compute the individual random forest models in parallel. To do this, the number of cores to use should be specified. To determine this, the parallel package can be used to understand how much parallelization can be done on your specific computer:</p>
<pre class="r"><code>cores &lt;- parallel::detectCores()
cores
#&gt; [1] 20</code></pre>
<p>To declare that parallel processing should be used, the <code>num.threads</code> for <code>ranger::ranger()</code> can be passed when setting the computational engine:</p>
<pre class="r"><code>rf_mod &lt;- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% 
  set_engine(&quot;ranger&quot;, num.threads = cores) %&gt;% 
  set_mode(&quot;classification&quot;)

rf_workflow &lt;- 
  workflow() %&gt;% 
  add_model(rf_mod) %&gt;% 
  add_recipe(rf_recipe)</code></pre>
<p>Again, if any other resampling method were used, it is better to parallel process in the more usual way.</p>
<p>To tune, a space-filling design with 25 candidate models is used:</p>
<pre class="r"><code>set.seed(3826)
rf_res &lt;- 
  rf_workflow %&gt;% 
  tune_grid(val_set,
            grid = 25,
            control = tune_ctrl,
            metrics = roc_only)
#&gt; i Creating pre-processing data to finalize unknown parameter: mtry
#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have column names if `.name_repair` is omitted as of tibble 2.0.0.
#&gt; Using compatibility `.name_repair`.
#&gt; This warning is displayed once every 8 hours.
#&gt; Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<p>The results, when plotted, indicates that both <code>mtry</code> and the minimum number of data points required to keep splitting should be fairly small (on average).</p>
<pre class="r"><code>autoplot(rf_res)</code></pre>
<p><img src="figs/rf-results-1.svg" width="672" /></p>
<p>If the model with the numerically best results are used, the final tuning parameter values would be:</p>
<pre class="r"><code>rf_best &lt;- select_best(rf_res, metric = &quot;roc_auc&quot;)
rf_best
#&gt; # A tibble: 1 x 2
#&gt;    mtry min_n
#&gt;   &lt;int&gt; &lt;int&gt;
#&gt; 1     5     3</code></pre>
<p>As before, the validation set ROC curve can be produced and overlaid with the previous logistic regression model:</p>
<pre class="r"><code>rf_auc &lt;- 
  rf_res %&gt;% 
  collect_predictions(parameters = rf_best) %&gt;% 
  roc_curve(children, .pred_children) %&gt;% 
  mutate(model = &quot;Random Forest&quot;)

bind_rows(rf_auc, lr_auc) %&gt;% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path() +
  geom_abline(lty = 2) + 
  coord_equal()</code></pre>
<p><img src="figs/rf-roc-curve-1.svg" width="672" /></p>
<p>The random forest is uniformly better across event probability thresholds.</p>
<p>If this model were chosen to be better than the other models, it can be used once again with <code>last_fit()</code> to fit the entire training set and then evaluate the test set.</p>
<p>However, the model object is redefined so that the <em>variable importance</em> scores are computed for this model. This should give some insight into which predictors are driving model performance.</p>
<pre class="r"><code>rf_mod &lt;- 
  rand_forest(mtry = 5, min_n = 3, trees = 1000) %&gt;% 
  set_engine(&quot;ranger&quot;, num.threads = cores, importance = &#39;impurity&#39;) %&gt;% 
  set_mode(&quot;classification&quot;)

rf_workflow &lt;- 
  workflow() %&gt;% 
  add_model(rf_mod) %&gt;% 
  add_recipe(rf_recipe)

rf_fit &lt;- rf_workflow %&gt;% last_fit(splits)
#&gt; ! Resample1: recipe: The `x` argument of `as_tibble.matrix()` must have column names ...</code></pre>
<p>From this fitted workflow, the vip package can be used to easily visualize the results:</p>
<pre class="r"><code>library(vip)

rf_fit$.workflow %&gt;% 
  pluck(1) %&gt;% 
  pull_workflow_fit() %&gt;% 
  vip(num_features = 20) </code></pre>
<p><img src="figs/rf-importance-1.svg" width="672" /></p>
<p>The most important predictors were the daily cost for the roo, the type of reservation, the time between the creation of the reservation and the arrival date, and the type of room that was reserved.</p>
</div>
<div id="test-set-results" class="section level1">
<h1>Test set results</h1>
<p>How did this model do on the test set? Was the validation set a good estimate of future performance?</p>
<pre class="r"><code>rf_fit %&gt;% 
  collect_predictions() %&gt;% 
  roc_auc(children, .pred_children)
#&gt; # A tibble: 1 x 3
#&gt;   .metric .estimator .estimate
#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 roc_auc binary         0.930

rf_fit %&gt;% 
  collect_predictions() %&gt;% 
  roc_curve(children, .pred_children) %&gt;% 
  autoplot()</code></pre>
<p><img src="figs/test-set-roc-curve-1.svg" width="672" /></p>
<p>Based on these results, the validation set and test set performance statistics are very close.</p>
</div>
<div id="session-information" class="section level1">
<h1>Session information</h1>
<pre><code>#&gt; ─ Session info ───────────────────────────────────────────────────────────────
#&gt;  setting  value                       
#&gt;  version  R version 3.6.1 (2019-07-05)
#&gt;  os       macOS Mojave 10.14.6        
#&gt;  system   x86_64, darwin15.6.0        
#&gt;  ui       X11                         
#&gt;  language (EN)                        
#&gt;  collate  en_US.UTF-8                 
#&gt;  ctype    en_US.UTF-8                 
#&gt;  tz       America/New_York            
#&gt;  date     2020-04-03                  
#&gt; 
#&gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&gt;  package    * version    date       lib source                             
#&gt;  broom      * 0.5.4      2020-01-27 [1] CRAN (R 3.6.0)                     
#&gt;  dials      * 0.0.6      2020-04-02 [1] local                              
#&gt;  dplyr      * 0.8.5      2020-03-07 [1] CRAN (R 3.6.0)                     
#&gt;  ggplot2    * 3.3.0      2020-03-05 [1] CRAN (R 3.6.0)                     
#&gt;  infer      * 0.5.1      2019-11-19 [1] CRAN (R 3.6.0)                     
#&gt;  parsnip    * 0.0.5.9001 2020-04-03 [1] Github (tidymodels/parsnip@0e83faf)
#&gt;  purrr      * 0.3.3      2019-10-18 [1] CRAN (R 3.6.0)                     
#&gt;  ranger       0.12.1     2020-01-10 [1] CRAN (R 3.6.0)                     
#&gt;  readr      * 1.3.1      2018-12-21 [1] CRAN (R 3.6.0)                     
#&gt;  recipes    * 0.1.10     2020-03-18 [1] CRAN (R 3.6.0)                     
#&gt;  rlang        0.4.5      2020-03-01 [1] CRAN (R 3.6.0)                     
#&gt;  rsample    * 0.0.6      2020-03-31 [1] local                              
#&gt;  tibble     * 3.0.0      2020-03-30 [1] CRAN (R 3.6.1)                     
#&gt;  tidymodels * 0.1.0      2020-02-16 [1] CRAN (R 3.6.0)                     
#&gt;  tune       * 0.1.0      2020-04-02 [1] CRAN (R 3.6.1)                     
#&gt;  vip        * 0.2.1      2020-01-20 [1] CRAN (R 3.6.0)                     
#&gt;  workflows  * 0.1.0      2019-12-30 [1] CRAN (R 3.6.1)                     
#&gt;  yardstick  * 0.0.5      2020-01-23 [1] CRAN (R 3.6.0)                     
#&gt; 
#&gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library</code></pre>
</div>
