---
title: "Using parsnip for Regression Models"
tags: [parsnip, glmnet, randomForest, ranger, regression models]
---

<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<script src="/rmarkdown-libs/elevate-section-attrs/elevate-section-attrs.js"></script>


<p>In addition to the <code>tidymodels</code> package, this example uses the <code>AmesHousing</code>, <code>glmnet</code>, <code>randomForest</code>, <code>ranger</code>, and <code>modeldata</code> packages.</p>
<p>The Ames housing data will be used to to demonstrate how regression models can be made using <code>parsnip</code>. We’ll create the data set and create a simple training/test set split:</p>
<pre class="r"><code>library(AmesHousing)
ames &lt;- make_ames()

library(tidymodels)

set.seed(4595)
data_split &lt;- initial_split(ames, strata = &quot;Sale_Price&quot;, p = 0.75)

ames_train &lt;- training(data_split)
ames_test  &lt;- testing(data_split)</code></pre>
<div id="random-forests" class="section level1">
<h1>Random Forests</h1>
<p>We’ll start by fitting a random forest model to a small set of parameters. Let’s say that the model would include predictors: <code>Longitude</code>, <code>Latitude</code>, <code>Lot_Area</code>, <code>Neighborhood</code>, and <code>Year_Sold</code>. A simple random forest model can be specified via</p>
<pre class="r"><code>rf_defaults &lt;- rand_forest(mode = &quot;regression&quot;)
rf_defaults
#&gt; Random Forest Model Specification (regression)</code></pre>
<p>The model will be fit with the <code>ranger</code> package. Since we didn’t add any extra arguments to <code>fit</code>, <em>many</em> of the arguments will be set to their defaults from the specific function that is used by <code>ranger::ranger</code>. The help pages for the model function describes the changes to the default parameters that are made and the <code>translate</code> function can also be used.</p>
<p><code>parsnip</code> gives two different interfaces to the models: the formula and non-formula interfaces. Let’s start with the non-formula interface:</p>
<pre class="r"><code>preds &lt;- c(&quot;Longitude&quot;, &quot;Latitude&quot;, &quot;Lot_Area&quot;, &quot;Neighborhood&quot;, &quot;Year_Sold&quot;)

rf_xy_fit &lt;- 
  rf_defaults %&gt;%
  set_engine(&quot;ranger&quot;) %&gt;%
  fit_xy(
    x = ames_train[, preds],
    y = log10(ames_train$Sale_Price)
  )
rf_xy_fit
#&gt; parsnip model object
#&gt; 
#&gt; Fit time:  927ms 
#&gt; Ranger result
#&gt; 
#&gt; Call:
#&gt;  ranger::ranger(formula = formula, data = data, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) 
#&gt; 
#&gt; Type:                             Regression 
#&gt; Number of trees:                  500 
#&gt; Sample size:                      2199 
#&gt; Number of independent variables:  5 
#&gt; Mtry:                             2 
#&gt; Target node size:                 5 
#&gt; Variable importance mode:         none 
#&gt; Splitrule:                        variance 
#&gt; OOB prediction error (MSE):       0.00843 
#&gt; R squared (OOB):                  0.736</code></pre>
<p>The non-formula interface doesn’t do anything to the predictors before giving it to the underlying model function. This particular model does <em>not</em> require indicator variables to be create prior to the model (note that the output shows “Number of independent variables: 5”).</p>
<p>For regression models, the basic <code>predict()</code> method can be used and returns a tibble with a column named <code>.pred</code>:</p>
<pre class="r"><code>test_results &lt;- ames_test %&gt;%
  select(Sale_Price) %&gt;%
  mutate(Sale_Price = log10(Sale_Price)) %&gt;%
  bind_cols(
    predict(rf_xy_fit, new_data = ames_test[, preds])
  )
test_results
#&gt; # A tibble: 731 x 2
#&gt;    Sale_Price .pred
#&gt;         &lt;dbl&gt; &lt;dbl&gt;
#&gt;  1       5.33  5.22
#&gt;  2       5.02  5.21
#&gt;  3       5.27  5.25
#&gt;  4       5.60  5.51
#&gt;  5       5.28  5.25
#&gt;  6       5.17  5.19
#&gt;  7       5.02  4.97
#&gt;  8       5.46  5.50
#&gt;  9       5.44  5.46
#&gt; 10       5.33  5.50
#&gt; # … with 721 more rows

# summarize performance
test_results %&gt;% metrics(truth = Sale_Price, estimate = .pred) 
#&gt; # A tibble: 3 x 3
#&gt;   .metric .estimator .estimate
#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 rmse    standard      0.0916
#&gt; 2 rsq     standard      0.715 
#&gt; 3 mae     standard      0.0663</code></pre>
<p>Note that:</p>
<ul>
<li>If the model required indicator variables, we would have to create them manually prior to using <code>fit()</code> (perhaps using the <code>recipes</code> package).</li>
<li>We had to manually log the outcome prior to modeling.</li>
</ul>
<p>Now, for illustration, let’s use the formula method using some new parameter values:</p>
<pre class="r"><code>rand_forest(mode = &quot;regression&quot;, mtry = 3, trees = 1000) %&gt;%
  set_engine(&quot;ranger&quot;) %&gt;%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
#&gt; parsnip model object
#&gt; 
#&gt; Fit time:  2.7s 
#&gt; Ranger result
#&gt; 
#&gt; Call:
#&gt;  ranger::ranger(formula = formula, data = data, mtry = ~3, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) 
#&gt; 
#&gt; Type:                             Regression 
#&gt; Number of trees:                  1000 
#&gt; Sample size:                      2199 
#&gt; Number of independent variables:  5 
#&gt; Mtry:                             3 
#&gt; Target node size:                 5 
#&gt; Variable importance mode:         none 
#&gt; Splitrule:                        variance 
#&gt; OOB prediction error (MSE):       0.00849 
#&gt; R squared (OOB):                  0.734</code></pre>
<p>Suppose that there was some feature in the <code>randomForest</code> package that we’d like to evaluate. To do so, the only part of the syntaxt that needs to change is the <code>set_engine</code> argument:</p>
<pre class="r"><code>rand_forest(mode = &quot;regression&quot;, mtry = 3, trees = 1000) %&gt;%
  set_engine(&quot;randomForest&quot;) %&gt;%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
#&gt; parsnip model object
#&gt; 
#&gt; Fit time:  1.9s 
#&gt; 
#&gt; Call:
#&gt;  randomForest(x = as.data.frame(x), y = y, ntree = ~1000, mtry = ~3) 
#&gt;                Type of random forest: regression
#&gt;                      Number of trees: 1000
#&gt; No. of variables tried at each split: 3
#&gt; 
#&gt;           Mean of squared residuals: 0.013
#&gt;                     % Var explained: 59.4</code></pre>
<p>Look at the formula code that was printed out, one function uses the argument name <code>ntree</code> and the other uses <code>num.trees</code>. <code>parsnip</code> doesn’t require you to know the specific names of the main arguments.</p>
<p>Now suppose that we want to modify the value of <code>mtry</code> based on the number of predictors in the data. Usually, the default value would be <code>floor(sqrt(num_predictors))</code>. To use a pure bagging model would require an <code>mtry</code> value equal to the total number of parameters. There may be cases where you may not know how many predictors are going to be present (perhaps due to the generation of indicator variables or a variable filter) so that might be difficult to know exactly.</p>
<p>When the model it being fit by <code>parsnip</code>, <a href="https://tidymodels.github.io/parsnip/reference/descriptors.html"><em>data descriptors</em></a> are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using <code>rand_forest()</code>), the values of the arguments that you give it are <em>immediately evaluated</em>… unless you delay them. To delay the evaluation of any argument, you can used <code>rlang::expr()</code> to make an expression.</p>
<p>Two relevant descriptors for what we are about to do are:</p>
<ul>
<li><code>.preds()</code>: the number of predictor <em>variables</em> in the data set that are associated with the predictors <strong>prior to dummy variable creation</strong>.</li>
<li><code>.cols()</code>: the number of predictor <em>columns</em> after dummy variables (or other encodings) are created.</li>
</ul>
<p>Since <code>ranger</code> won’t create indicator values, <code>.preds()</code> would be appropriate for using <code>mtry</code> for a bagging model.</p>
<p>For example, let’s use an expression with the <code>.preds()</code> descriptor to fit a bagging model:</p>
<pre class="r"><code>rand_forest(mode = &quot;regression&quot;, mtry = .preds(), trees = 1000) %&gt;%
  set_engine(&quot;ranger&quot;) %&gt;%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
#&gt; parsnip model object
#&gt; 
#&gt; Fit time:  3.6s 
#&gt; Ranger result
#&gt; 
#&gt; Call:
#&gt;  ranger::ranger(formula = formula, data = data, mtry = ~.preds(),      num.trees = ~1000, num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) 
#&gt; 
#&gt; Type:                             Regression 
#&gt; Number of trees:                  1000 
#&gt; Sample size:                      2199 
#&gt; Number of independent variables:  5 
#&gt; Mtry:                             5 
#&gt; Target node size:                 5 
#&gt; Variable importance mode:         none 
#&gt; Splitrule:                        variance 
#&gt; OOB prediction error (MSE):       0.0087 
#&gt; R squared (OOB):                  0.728</code></pre>
</div>
<div id="penalized-logistic-regression" class="section level1">
<h1>Penalized Logistic Regression</h1>
<p>A linear model might work here too. The <code>linear_reg</code> model can be used. To use regularization/penalization, there are two engines that can do that here: the <code>glmnet</code> and <code>sparklyr</code> packages. The former will be used here and it only implements the non-formula method. <code>parsnip</code> will allow either to be used though.</p>
<p>When regularization is used, the predictors should first be centered and scaled before given to the model. The formula method won’t do that so some other methods will be required. We’ll use <code>recipes</code> package for that (more information <a href="https://tidymodels.github.io/recipes/">here</a>).</p>
<pre class="r"><code>norm_recipe &lt;- 
  recipe(
    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  ) %&gt;%
  step_other(Neighborhood) %&gt;% 
  step_dummy(all_nominal()) %&gt;%
  step_center(all_predictors()) %&gt;%
  step_scale(all_predictors()) %&gt;%
  step_log(Sale_Price, base = 10) %&gt;% 
  # estimate the means and standard deviations
  prep(training = ames_train, retain = TRUE)

# Now let&#39;s fit the model using the processed version of the data

glmn_fit &lt;- 
  linear_reg(penalty = 0.001, mixture = 0.5) %&gt;% 
  set_engine(&quot;glmnet&quot;) %&gt;%
  fit(Sale_Price ~ ., data = juice(norm_recipe))
glmn_fit
#&gt; parsnip model object
#&gt; 
#&gt; Fit time:  8ms 
#&gt; 
#&gt; Call:  glmnet::glmnet(x = as.matrix(x), y = y, family = &quot;gaussian&quot;,      alpha = ~0.5) 
#&gt; 
#&gt;    Df  %Dev Lambda
#&gt; 1   0 0.000 0.1370
#&gt; 2   1 0.019 0.1250
#&gt; 3   1 0.036 0.1140
#&gt; 4   1 0.050 0.1040
#&gt; 5   2 0.068 0.0946
#&gt; 6   4 0.093 0.0862
#&gt; 7   5 0.125 0.0785
#&gt; 8   5 0.153 0.0716
#&gt; 9   7 0.184 0.0652
#&gt; 10  7 0.214 0.0594
#&gt; 11  7 0.240 0.0541
#&gt; 12  8 0.262 0.0493
#&gt; 13  8 0.286 0.0449
#&gt; 14  8 0.306 0.0409
#&gt; 15  8 0.323 0.0373
#&gt; 16  8 0.338 0.0340
#&gt; 17  8 0.350 0.0310
#&gt; 18  8 0.361 0.0282
#&gt; 19  9 0.370 0.0257
#&gt; 20  9 0.379 0.0234
#&gt; 21  9 0.386 0.0213
#&gt; 22  9 0.392 0.0195
#&gt; 23  9 0.397 0.0177
#&gt; 24  9 0.401 0.0161
#&gt; 25  9 0.405 0.0147
#&gt; 26  9 0.408 0.0134
#&gt; 27 10 0.410 0.0122
#&gt; 28 11 0.413 0.0111
#&gt; 29 11 0.415 0.0101
#&gt; 30 11 0.417 0.0092
#&gt; 31 12 0.418 0.0084
#&gt; 32 12 0.420 0.0077
#&gt; 33 12 0.421 0.0070
#&gt; 34 12 0.422 0.0064
#&gt; 35 12 0.423 0.0058
#&gt; 36 12 0.423 0.0053
#&gt; 37 12 0.424 0.0048
#&gt; 38 12 0.425 0.0044
#&gt; 39 12 0.425 0.0040
#&gt; 40 12 0.425 0.0036
#&gt; 41 12 0.426 0.0033
#&gt; 42 12 0.426 0.0030
#&gt; 43 12 0.426 0.0028
#&gt; 44 12 0.426 0.0025
#&gt; 45 12 0.426 0.0023
#&gt; 46 12 0.426 0.0021
#&gt; 47 12 0.427 0.0019
#&gt; 48 12 0.427 0.0017
#&gt; 49 12 0.427 0.0016
#&gt; 50 12 0.427 0.0014
#&gt; 51 12 0.427 0.0013
#&gt; 52 12 0.427 0.0012
#&gt; 53 12 0.427 0.0011
#&gt; 54 12 0.427 0.0010
#&gt; 55 12 0.427 0.0009
#&gt; 56 12 0.427 0.0008
#&gt; 57 12 0.427 0.0008
#&gt; 58 12 0.427 0.0007
#&gt; 59 12 0.427 0.0006
#&gt; 60 12 0.427 0.0006
#&gt; 61 12 0.427 0.0005
#&gt; 62 12 0.427 0.0005
#&gt; 63 12 0.427 0.0004
#&gt; 64 12 0.427 0.0004
#&gt; 65 12 0.427 0.0004</code></pre>
<p>If <code>penalty</code> were not specified, all of the <code>lambda</code> values would be computed.</p>
<p>To get the predictions for this specific value of <code>lambda</code> (aka <code>penalty</code>):</p>
<pre class="r"><code># First, get the processed version of the test set predictors:
test_normalized &lt;- bake(norm_recipe, new_data = ames_test, all_predictors())

test_results &lt;- 
  test_results %&gt;%
  rename(`random forest` = .pred) %&gt;%
  bind_cols(
    predict(glmn_fit, new_data = test_normalized) %&gt;%
      rename(glmnet = .pred)
  )
test_results
#&gt; # A tibble: 731 x 3
#&gt;    Sale_Price `random forest` glmnet
#&gt;         &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;
#&gt;  1       5.33            5.22   5.27
#&gt;  2       5.02            5.21   5.17
#&gt;  3       5.27            5.25   5.23
#&gt;  4       5.60            5.51   5.25
#&gt;  5       5.28            5.25   5.25
#&gt;  6       5.17            5.19   5.19
#&gt;  7       5.02            4.97   5.19
#&gt;  8       5.46            5.50   5.49
#&gt;  9       5.44            5.46   5.48
#&gt; 10       5.33            5.50   5.47
#&gt; # … with 721 more rows

test_results %&gt;% metrics(truth = Sale_Price, estimate = glmnet) 
#&gt; # A tibble: 3 x 3
#&gt;   .metric .estimator .estimate
#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 rmse    standard      0.132 
#&gt; 2 rsq     standard      0.410 
#&gt; 3 mae     standard      0.0956

test_results %&gt;% 
  gather(model, prediction, -Sale_Price) %&gt;% 
  ggplot(aes(x = prediction, y = Sale_Price)) + 
  geom_abline(col = &quot;green&quot;, lty = 2) + 
  geom_point(alpha = .4) + 
  facet_wrap(~model) + 
  coord_fixed()</code></pre>
<p><img src="/examples/parsnip-regression-example/index_files/figure-html/glmn-pred-1.png" width="672" /></p>
</div>
