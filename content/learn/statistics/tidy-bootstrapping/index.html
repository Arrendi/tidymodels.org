---
title: "Bootstrap resampling and tidy regression models"
tags: [rsample, broom]
categories: [statistical analysis, resampling]
type: learn-subsection
---



<div id="learning-objective" class="section level1">
<h1>Learning objective</h1>
<p>Apply bootstrap resampling to estimate model parameters</p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This article only requires that the tidymodels package be installed.</p>
<p>Combining fitted models in a tidy way becomes useful when performing bootstrapping or permutation tests. These approaches have been explored before, for instance by <a href="http://rstudio-pubs-static.s3.amazonaws.com/19698_a4c472606e3c43e4b94720506e49bb7b.html">Andrew MacDonald here</a>, and <a href="https://github.com/hadley/dplyr/issues/269">Hadley has explored efficient support for bootstrapping</a> as a potential enhancement to dplyr. The tidymodels package <a href="https://broom.tidyverse.org/">broom</a> fits naturally with dplyr in performing these analyses.</p>
<p>Bootstrapping consists of randomly sampling a data set with replacement, then performing the analysis individually on each bootstrapped replicate. The variation in the resulting estimate is then a reasonable approximation of the variance in our estimate.</p>
<p>Let’s say we want to fit a nonlinear model to the weight/mileage relationship in the <code>mtcars</code> data set.</p>
<pre class="r"><code>library(tidymodels)

ggplot(mtcars, aes(mpg, wt)) + 
    geom_point()</code></pre>
<p><img src="figs/unnamed-chunk-1-1.svg" width="672" /></p>
<p>We might use the method of nonlinear least squares (via the <code>nls()</code> function) to fit a model.</p>
<pre class="r"><code>nlsfit &lt;- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))
summary(nlsfit)
#&gt; 
#&gt; Formula: mpg ~ k/wt + b
#&gt; 
#&gt; Parameters:
#&gt;   Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; k    45.83       4.25   10.79  7.6e-12 ***
#&gt; b     4.39       1.54    2.85   0.0077 ** 
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 2.77 on 30 degrees of freedom
#&gt; 
#&gt; Number of iterations to convergence: 1 
#&gt; Achieved convergence tolerance: 2.88e-08

ggplot(mtcars, aes(wt, mpg)) +
    geom_point() +
    geom_line(aes(y = predict(nlsfit)))</code></pre>
<p><img src="figs/unnamed-chunk-2-1.svg" width="672" /></p>
<p>While this does provide a p-value and confidence intervals for the parameters, these are based on model assumptions that may not hold in real data. Bootstrapping is a popular method for providing confidence intervals and predictions that are more robust to the nature of the data.</p>
</div>
<div id="bootstrapping-models" class="section level1">
<h1>Bootstrapping models</h1>
<p>We can use the <code>bootstraps()</code> function in the rsample package to sample bootstrap replications. First, we construct 100 bootstrap replications of the data, each of which has been randomly sampled with replacement. The resulting object is an <code>rset</code>, which is a data frame with a column of <code>rsplit</code> objects.</p>
<p>An <code>rsplit</code> object has two main components: an analysis data set and an assessment data set, accessible via <code>analysis(rsplit)</code> and <code>assessment(rsplit)</code> respectively. For bootstrap samples, the analysis data set is the bootstrap sample itself, and the assessment data set consists of all the out of bag samples.</p>
<pre class="r"><code>set.seed(27)
boots &lt;- bootstraps(mtcars, times = 2000, apparent = TRUE)
boots
#&gt; # Bootstrap sampling with apparent sample 
#&gt; # A tibble: 2,001 x 2
#&gt;    splits          id           
#&gt;    &lt;list&gt;          &lt;chr&gt;        
#&gt;  1 &lt;split [32/13]&gt; Bootstrap0001
#&gt;  2 &lt;split [32/10]&gt; Bootstrap0002
#&gt;  3 &lt;split [32/13]&gt; Bootstrap0003
#&gt;  4 &lt;split [32/11]&gt; Bootstrap0004
#&gt;  5 &lt;split [32/9]&gt;  Bootstrap0005
#&gt;  6 &lt;split [32/10]&gt; Bootstrap0006
#&gt;  7 &lt;split [32/11]&gt; Bootstrap0007
#&gt;  8 &lt;split [32/13]&gt; Bootstrap0008
#&gt;  9 &lt;split [32/11]&gt; Bootstrap0009
#&gt; 10 &lt;split [32/11]&gt; Bootstrap0010
#&gt; # … with 1,991 more rows</code></pre>
<p>We create a helper function to fit an <code>nls()</code> model on each bootstrap sample, and then use <code>purrr::map()</code> to apply this function to all the bootstrap samples at once. Similarly, we create a column of tidy coefficient information by unnesting.</p>
<pre class="r"><code>fit_nls_on_bootstrap &lt;- function(split) {
    nls(mpg ~ k / wt + b, analysis(split), start = list(k = 1, b = 0))
}

boot_models &lt;-
  boots %&gt;% 
  mutate(model = map(splits, fit_nls_on_bootstrap),
         coef_info = map(model, tidy))

boot_coefs &lt;- 
  boot_models %&gt;% 
  unnest(coef_info)</code></pre>
<p>The unnested coefficient information contains a summary of each replication combined in a single data frame:</p>
<pre class="r"><code>boot_coefs
#&gt; # A tibble: 4,002 x 8
#&gt;    splits         id           model term  estimate std.error statistic  p.value
#&gt;    &lt;list&gt;         &lt;chr&gt;        &lt;lis&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
#&gt;  1 &lt;split [32/13… Bootstrap00… &lt;nls&gt; k        42.1       4.05     10.4  1.91e-11
#&gt;  2 &lt;split [32/13… Bootstrap00… &lt;nls&gt; b         5.39      1.43      3.78 6.93e- 4
#&gt;  3 &lt;split [32/10… Bootstrap00… &lt;nls&gt; k        49.9       5.66      8.82 7.82e-10
#&gt;  4 &lt;split [32/10… Bootstrap00… &lt;nls&gt; b         3.73      1.92      1.94 6.13e- 2
#&gt;  5 &lt;split [32/13… Bootstrap00… &lt;nls&gt; k        37.8       2.68     14.1  9.01e-15
#&gt;  6 &lt;split [32/13… Bootstrap00… &lt;nls&gt; b         6.73      1.17      5.75 2.78e- 6
#&gt;  7 &lt;split [32/11… Bootstrap00… &lt;nls&gt; k        45.6       4.45     10.2  2.70e-11
#&gt;  8 &lt;split [32/11… Bootstrap00… &lt;nls&gt; b         4.75      1.62      2.93 6.38e- 3
#&gt;  9 &lt;split [32/9]&gt; Bootstrap00… &lt;nls&gt; k        43.6       4.63      9.41 1.85e-10
#&gt; 10 &lt;split [32/9]&gt; Bootstrap00… &lt;nls&gt; b         5.89      1.68      3.51 1.44e- 3
#&gt; # … with 3,992 more rows</code></pre>
</div>
<div id="bootstrap-confidence-intervals" class="section level1">
<h1>Bootstrap confidence intervals</h1>
<p>We can then calculate confidence intervals (using what is called the <a href="https://www.uvm.edu/~dhowell/StatPages/Randomization%20Tests/ResamplingWithR/BootstMeans/bootstrapping_means.html">percentile method</a>):</p>
<pre class="r"><code>percentile_intervals &lt;- int_pctl(boot_models, coef_info)
percentile_intervals
#&gt; # A tibble: 2 x 6
#&gt;   term   .lower .estimate .upper .alpha .method   
#&gt;   &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     
#&gt; 1 b      0.0475      4.12   7.31   0.05 percentile
#&gt; 2 k     37.6        46.7   59.8    0.05 percentile</code></pre>
<p>Or we can use histograms to get a more detailed idea of the uncertainty in each estimate:</p>
<pre class="r"><code>ggplot(boot_coefs, aes(estimate)) +
  geom_histogram(bins = 30) +
  facet_wrap( ~ term, scales = &quot;free&quot;) +
  geom_vline(aes(xintercept = .lower), data = percentile_intervals, col = &quot;blue&quot;) +
  geom_vline(aes(xintercept = .upper), data = percentile_intervals, col = &quot;blue&quot;)</code></pre>
<p><img src="figs/unnamed-chunk-6-1.svg" width="672" /></p>
<p>rsample also has functions for other types of confidence intervals.</p>
</div>
<div id="showing-possible-model-fits" class="section level1">
<h1>Showing possible model fits</h1>
<p>Or we can use <code>augment()</code> to visualize the uncertainty in the curve. Since there are so many bootstrap samples, we’ll show a sample of the model fits:</p>
<pre class="r"><code>boot_aug &lt;- 
  boot_models %&gt;% 
  sample_n(200) %&gt;% 
  mutate(augmented = map(model, augment)) %&gt;% 
  unnest(augmented)

boot_aug
#&gt; # A tibble: 6,400 x 8
#&gt;    splits         id            model coef_info         mpg    wt .fitted .resid
#&gt;    &lt;list&gt;         &lt;chr&gt;         &lt;lis&gt; &lt;list&gt;          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
#&gt;  1 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  16.4  4.07    15.6  0.829
#&gt;  2 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  19.7  2.77    21.9 -2.21 
#&gt;  3 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  19.2  3.84    16.4  2.84 
#&gt;  4 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  21.4  2.78    21.8 -0.437
#&gt;  5 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  26    2.14    27.8 -1.75 
#&gt;  6 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  33.9  1.84    32.0  1.88 
#&gt;  7 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  32.4  2.2     27.0  5.35 
#&gt;  8 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  30.4  1.62    36.1 -5.70 
#&gt;  9 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  21.5  2.46    24.4 -2.86 
#&gt; 10 &lt;split [32/11… Bootstrap1644 &lt;nls&gt; &lt;tibble [2 × 5…  26    2.14    27.8 -1.75 
#&gt; # … with 6,390 more rows</code></pre>
<pre class="r"><code>ggplot(boot_aug, aes(wt, mpg)) +
  geom_line(aes(y = .fitted, group = id), alpha = .2, col = &quot;blue&quot;) +
  geom_point()</code></pre>
<p><img src="figs/unnamed-chunk-8-1.svg" width="672" /></p>
<p>With only a few small changes, we could easily perform bootstrapping with other kinds of predictive or hypothesis testing models, since the <code>tidy()</code> and <code>augment()</code> functions works for many statistical outputs. As another example, we could use <code>smooth.spline()</code>, which fits a cubic smoothing spline to data:</p>
<pre class="r"><code>fit_spline_on_bootstrap &lt;- function(split) {
    data &lt;- analysis(split)
    smooth.spline(data$wt, data$mpg, df = 4)
}

boot_splines &lt;- 
  boots %&gt;% 
  sample_n(200) %&gt;% 
  mutate(spline = map(splits, fit_spline_on_bootstrap),
         aug_train = map(spline, augment))

splines_aug &lt;- 
  boot_splines %&gt;% 
  unnest(aug_train)

ggplot(splines_aug, aes(x, y)) +
  geom_line(aes(y = .fitted, group = id), alpha = 0.2, col = &quot;blue&quot;) +
  geom_point()</code></pre>
<p><img src="figs/unnamed-chunk-9-1.svg" width="672" /></p>
</div>
<div id="session-information" class="section level1">
<h1>Session information</h1>
<pre><code>#&gt; ─ Session info ───────────────────────────────────────────────────────────────
#&gt;  setting  value                       
#&gt;  version  R version 3.6.2 (2019-12-12)
#&gt;  os       macOS Mojave 10.14.6        
#&gt;  system   x86_64, darwin15.6.0        
#&gt;  ui       X11                         
#&gt;  language (EN)                        
#&gt;  collate  en_US.UTF-8                 
#&gt;  ctype    en_US.UTF-8                 
#&gt;  tz       America/Denver              
#&gt;  date     2020-04-01                  
#&gt; 
#&gt; ─ Packages ───────────────────────────────────────────────────────────────────
#&gt;  package    * version      date       lib source                               
#&gt;  broom      * 0.5.5        2020-02-29 [1] CRAN (R 3.6.0)                       
#&gt;  dials      * 0.0.4.9000   2020-03-20 [1] local                                
#&gt;  dplyr      * 0.8.5        2020-03-07 [1] CRAN (R 3.6.0)                       
#&gt;  ggplot2    * 3.3.0        2020-03-05 [1] CRAN (R 3.6.0)                       
#&gt;  infer      * 0.5.1        2019-11-19 [1] CRAN (R 3.6.0)                       
#&gt;  parsnip    * 0.0.5        2020-01-07 [1] CRAN (R 3.6.2)                       
#&gt;  purrr      * 0.3.3        2019-10-18 [1] CRAN (R 3.6.0)                       
#&gt;  recipes    * 0.1.10       2020-03-18 [1] CRAN (R 3.6.0)                       
#&gt;  rlang        0.4.5.9000   2020-03-20 [1] Github (r-lib/rlang@a90b04b)         
#&gt;  rsample    * 0.0.5        2019-07-12 [1] CRAN (R 3.6.0)                       
#&gt;  tibble     * 2.99.99.9014 2020-03-26 [1] Github (tidyverse/tibble@2448bb0)    
#&gt;  tidymodels * 0.1.0        2020-02-16 [1] CRAN (R 3.6.0)                       
#&gt;  tune       * 0.0.1.9000   2020-03-23 [1] local                                
#&gt;  workflows  * 0.1.1.9000   2020-03-20 [1] Github (tidymodels/workflows@e995c18)
#&gt;  yardstick  * 0.0.5.9000   2020-03-20 [1] Github (tidymodels/yardstick@6a9a69a)
#&gt; 
#&gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library</code></pre>
</div>
